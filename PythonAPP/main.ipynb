{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Rapid Prototyping\n",
    "### Start every session by importing the Modules and initializing the tools and establishing a connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import re\n",
    "import Modules.DBInterface as DBInterface\n",
    "import Modules.jsonImporter as jsonImporter\n",
    "import Modules.Interface as Interface\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm.autonotebook import tqdm\n",
    "from somajo import SoMaJo\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "import numpy\n",
    "\n",
    "ROOT_ID = 0\n",
    "DOC_ID = 1\n",
    "SENTENCE_ID = 2\n",
    "PARAGRAPH_ID = 3\n",
    "YEAR_ID = 4\n",
    "AUTHOR_ID = 5\n",
    "CAT_ID = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The parameters for the DBInterface are, in order, DBName, username, password. Change those accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datbase connection successful. Database is ('PostgreSQL 12.9 (Ubuntu 12.9-0ubuntu0.20.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0, 64-bit',)\n",
      "Importing Base Tables...\n",
      "Preparing joined tables...\n"
     ]
    }
   ],
   "source": [
    "DB = DBInterface.DBInterface('speeches', 'postgres', 'test')\n",
    "DB.connect()\n",
    "filepath = \"../data/speeches\"\n",
    "IF = Interface.Interface(DB,filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "#### Use the createDB command to create and connect to a new DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created successfully........\n"
     ]
    }
   ],
   "source": [
    "DB.createDB(\"speeches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### The following field imports a part of the 'speeches' dataset, including metadata and splitting texts into paragraphs and sentences. Potentially change the filepath accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AA = '../data/speeches/AuswärtigesAmt.xml'\n",
    "BR = '../data/speeches/Bundesregierung.xml'\n",
    "BP = '../data/speeches/Bundespräsidenten.xml'\n",
    "BTP = '../data/speeches/Bundestagspräsidenten.xml'\n",
    "\n",
    "\n",
    "all = []\n",
    "\n",
    "kanzler = [\"Gerhard Schröder\", \"Angela Merkel\", \"Helmut Kohl\"]\n",
    "\n",
    "AA_list = []\n",
    "BR_list = []\n",
    "BP_list = []\n",
    "BTP_list = []\n",
    "\n",
    "tree = ET.parse(AA)\n",
    "root = tree.getroot()\n",
    "\n",
    "for i in root:\n",
    "    date = i.attrib[\"datum\"]\n",
    "    if date[:4].isnumeric():\n",
    "        date = date[:4]\n",
    "    else:\n",
    "        date = date[-4:]\n",
    "        \n",
    "    AA_list.append([i.attrib[\"person\"], i.attrib[\"titel\"], date, i[0].text, \"Auswärtiges Amt\"])\n",
    "\n",
    "tree = ET.parse(BR)\n",
    "root = tree.getroot()\n",
    "for i in root:\n",
    "    if i.attrib[\"person\"] in kanzler:\n",
    "        date = i.attrib[\"datum\"]\n",
    "        if date[:4].isnumeric():\n",
    "            date = date[:4]\n",
    "        else:\n",
    "            date = date[-4:]\n",
    "\n",
    "        BR_list.append([i.attrib[\"person\"], i.attrib[\"titel\"], date, i[0].text, \"Bundesregierung\"])\n",
    "\n",
    "tree = ET.parse(BP)\n",
    "root = tree.getroot()\n",
    "for i in root:\n",
    "    date = i.attrib[\"datum\"]\n",
    "    if date[:4].isnumeric():\n",
    "        date = date[:4]\n",
    "    else:\n",
    "        date = date[-4:]\n",
    "        \n",
    "    BP_list.append([i.attrib[\"person\"], i.attrib[\"titel\"], date, i[0].text, \"Bundespräsident\"])\n",
    "\n",
    "tree = ET.parse(BTP)\n",
    "root = tree.getroot()\n",
    "for i in root:\n",
    "    date = i.attrib[\"datum\"]\n",
    "    if date[:4].isnumeric():\n",
    "        date = date[:4]\n",
    "    else:\n",
    "        date = date[-4:]\n",
    "        \n",
    "    BTP_list.append([i.attrib[\"person\"], i.attrib[\"titel\"], date, i[0].text, \"Bundestagspräsident\"])\n",
    "    \n",
    "all.extend(AA_list)\n",
    "all.extend(BR_list)\n",
    "all.extend(BP_list)\n",
    "#all.extend(BTP_list)\n",
    "\n",
    "speeches = pd.DataFrame(all)\n",
    "\n",
    "term_id = 7\n",
    "allTerms = []\n",
    "allSubs = []\n",
    "for i in speeches[0]:\n",
    "    tmpList = [term_id, i, \"dev\", False]\n",
    "    tmpListSub = [term_id, AUTHOR_ID, \"init\", 100]\n",
    "    term_id += 1\n",
    "    allTerms.append(tmpList)\n",
    "    allSubs.append(tmpListSub)\n",
    "for i in speeches[2].unique():\n",
    "    tmpList = [term_id, i, \"dev\", False]\n",
    "    tmpListSub = [term_id, YEAR_ID, \"init\", 100]\n",
    "    term_id += 1\n",
    "    allTerms.append(tmpList)\n",
    "    allSubs.append(tmpListSub)\n",
    "for i in speeches[1].unique():\n",
    "    tmpList = [term_id, i, \"dev\", False]\n",
    "    tmpListSub = [term_id, DOC_ID, \"init\", 100]\n",
    "    term_id += 1\n",
    "    allTerms.append(tmpList)\n",
    "    allSubs.append(tmpListSub)\n",
    "for i in speeches[4].unique():\n",
    "    tmpList = [term_id, i, \"dev\", False]\n",
    "    tmpListSub = [term_id, CAT_ID, \"init\", 100]\n",
    "    term_id += 1\n",
    "    allTerms.append(tmpList)\n",
    "    allSubs.append(tmpListSub)\n",
    "\n",
    "\n",
    "allTerms = pd.DataFrame(allTerms)\n",
    "allSubs = pd.DataFrame(allSubs)\n",
    "allTerms = allTerms.rename(columns={0 : \"id\", 1 : \"name\", 2 : \"knowledge_base\", 3 : \"deleted\"})\n",
    "allSubs = allSubs.rename(columns={0 : \"term\", 1 : \"parent\", 2 : \"representation\", 3 : \"confidence\"})\n",
    "allTerms[\"id\"] = pd.to_numeric(allTerms[\"id\"])\n",
    "allTerms.drop_duplicates(subset=['name'], inplace=True)\n",
    "allTerms[\"name\"].is_unique\n",
    "termDict = dict(zip(allTerms[\"name\"], allTerms[\"id\"]))\n",
    "speeches[\"author_id\"] = speeches[0].map(allTerms.set_index(\"name\")[\"id\"])\n",
    "speeches['title_id'] = speeches[1].map(allTerms.set_index(\"name\")[\"id\"])\n",
    "speeches['year_id'] = speeches[2].map(allTerms.set_index(\"name\")[\"id\"])\n",
    "speeches['cat_id'] = speeches[4].map(allTerms.set_index(\"name\")[\"id\"])\n",
    "\n",
    "#Create Base DB Layout\n",
    "engine = create_engine('postgresql+psycopg2://postgres:test@localhost/speeches')\n",
    "engine.connect()\n",
    "\n",
    "register_adapter(numpy.float64, addapt_numpy_float64)\n",
    "register_adapter(numpy.int64, addapt_numpy_int64)\n",
    "\n",
    "DB = DBInterface.DBInterface('speeches', 'postgres', 'test')\n",
    "DB.connect()\n",
    "cur = DB.conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"TRUNCATE public.terms\"\"\")\n",
    "cur.execute(\"\"\"TRUNCATE public.subsumptions\"\"\")\n",
    "cur.execute(\"\"\"TRUNCATE public.spans\"\"\")\n",
    "cur.execute(\"\"\"TRUNCATE public.intersections\"\"\")\n",
    "cur.execute(\"\"\"TRUNCATE public.full_texts\"\"\")\n",
    "cur.execute(\"\"\"TRUNCATE public.wiki\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"INSERT into public.terms values({},'{}', '{}', {});\"\"\".format(0, \"root\", \"dev\", False))\n",
    "cur.execute(\"\"\"INSERT into public.terms values({},'{}', '{}', {});\"\"\".format(1, \"Dokumente\", \"dev\", False))\n",
    "cur.execute(\"\"\"INSERT into public.terms values({},'{}', '{}', {});\"\"\".format(2, \"Sätze\", \"dev\", False))\n",
    "cur.execute(\"\"\"INSERT into public.terms values({},'{}', '{}', {});\"\"\".format(3, \"Paragraphen\", \"dev\", False))\n",
    "cur.execute(\"\"\"INSERT into public.terms values({},'{}', '{}', {});\"\"\".format(4, \"Jahr\", \"dev\", False))\n",
    "cur.execute(\"\"\"INSERT into public.terms values({},'{}', '{}', {});\"\"\".format(5, \"Sprecher\", \"dev\", False))\n",
    "cur.execute(\"\"\"INSERT into public.terms values({},'{}', '{}', {});\"\"\".format(6, \"Behörde\", \"dev\", False))\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"INSERT into public.subsumptions values({},{}, '{}', {});\"\"\".format(1, 0, \"init\", 100))\n",
    "cur.execute(\"\"\"INSERT into public.subsumptions values({},{}, '{}', {});\"\"\".format(2, 0, \"init\", 100))\n",
    "cur.execute(\"\"\"INSERT into public.subsumptions values({},{}, '{}', {});\"\"\".format(3, 0, \"init\", 100))\n",
    "cur.execute(\"\"\"INSERT into public.subsumptions values({},{}, '{}', {});\"\"\".format(4, 0, \"init\", 100))\n",
    "cur.execute(\"\"\"INSERT into public.subsumptions values({},{}, '{}', {});\"\"\".format(5, 0, \"init\", 100))\n",
    "cur.execute(\"\"\"INSERT into public.subsumptions values({},{}, '{}', {});\"\"\".format(6, 0, \"init\", 100))\n",
    "\n",
    "DB.conn.commit()\n",
    "\n",
    "print(\"terms...\")\n",
    "allTerms.to_sql('terms', con=engine, if_exists='append', method='multi', chunksize = 100000, index = False)\n",
    "print(\"subs...\")\n",
    "allSubs.to_sql('subsumptions', con=engine, if_exists='append', method='multi', chunksize = 100000, index = False)\n",
    "\n",
    "def addapt_numpy_float64(numpy_float64):\n",
    "    return AsIs(numpy_float64)\n",
    "\n",
    "def addapt_numpy_int64(numpy_int64):\n",
    "    return AsIs(numpy_int64)\n",
    "\n",
    "termID = allTerms[\"id\"].values.max() + 1\n",
    "spanID = 1 \n",
    "refID = 0\n",
    "spanList = []\n",
    "intersecList = []\n",
    "refList = []\n",
    "sentenceTerms = []\n",
    "sentenceSubs = []\n",
    "paras = {}\n",
    "sents = {}\n",
    "\n",
    "spanDF = pd.DataFrame()\n",
    "intersecDF = pd.DataFrame()\n",
    "refDF = pd.DataFrame()\n",
    "sentTermDF = pd.DataFrame()\n",
    "sentSubDF = pd.DataFrame()\n",
    "        \n",
    "for index, row in tqdm(speeches.iterrows(), total=len(speeches)):\n",
    "    docID = row[\"title_id\"]\n",
    "    yearID = row[\"year_id\"]\n",
    "    categoryID = row[\"cat_id\"]\n",
    "    authorID = row[\"author_id\"]\n",
    "\n",
    "    spanBegin = 0\n",
    "    \n",
    "    text = row[3].split(\"\\n\")\n",
    "    paragraphs = [i for i in text if i != \"\"]\n",
    "    \n",
    "    docSpan = spanID\n",
    "    spanID += 1\n",
    "    yearSpan = spanID\n",
    "    spanID += 1\n",
    "    catSpan = spanID\n",
    "    spanID += 1\n",
    "    authorSpan = spanID\n",
    "    spanID += 1\n",
    "    \n",
    "    intersecs = [[docSpan, yearSpan], [yearSpan, docSpan], \n",
    "                 [docSpan, catSpan], [catSpan, docSpan], \n",
    "                 [catSpan, yearSpan], [yearSpan, catSpan],\n",
    "                 [docSpan, authorSpan], [authorSpan, docSpan], \n",
    "                 [authorSpan, catSpan], [catSpan, authorSpan], \n",
    "                 [authorSpan, yearSpan], [yearSpan, authorSpan]]\n",
    "    intersecList.extend(intersecs)\n",
    "    \n",
    "    tokenizer = SoMaJo(\"de_CMC\", split_camel_case=True)\n",
    "    \n",
    "    breakCount = 0\n",
    "    docTMP = \"\"\n",
    "    for i in paragraphs:\n",
    "        paraSpan = spanID\n",
    "        spanID += 1\n",
    "        paraTMP = \"\"\n",
    "        tmpLen = 0\n",
    "        sentences = tokenizer.tokenize_text([i])\n",
    "        for i in sentences:\n",
    "            tmp = [j.text for j in i]\n",
    "            tmp = TreebankWordDetokenizer().detokenize(tmp)\n",
    "            if len(tmp) > 5:\n",
    "                sentID = termID\n",
    "                if sents.get(str(tmp)) == None:\n",
    "                    sents[str(tmp)] = termID\n",
    "                    sentenceTerms.append(tuple([sentID, str(tmp), \"dev\", False]))\n",
    "                    sentenceSubs.append(tuple([sentID, SENTENCE_ID]))\n",
    "                    termID += 1\n",
    "                else:\n",
    "                    sentID = sents.get(str(tmp))\n",
    "\n",
    "                spanList.append(tuple([sentID, len(docTMP) + len(paraTMP) + 2, len(docTMP) + len(paraTMP) + len(tmp) + 2, refID, spanID, str(tmp), False]))\n",
    "                intersecs = [[docSpan, spanID], [spanID, docSpan], [spanID, yearSpan], \n",
    "                             [yearSpan, spanID], [spanID, catSpan], [catSpan, spanID],\n",
    "                            [authorSpan, spanID], [spanID, authorSpan],\n",
    "                            [paraSpan, spanID], [spanID, paraSpan]]\n",
    "\n",
    "                spanID += 1\n",
    "                intersecList.extend(intersecs)\n",
    "            paraTMP += \" \" + tmp\n",
    "            tmpLen += len(str(tmp)) + 1\n",
    "        \n",
    "        if len(paraTMP) > 5:\n",
    "            paraTMPLabel = paraTMP\n",
    "            if len(paraTMPLabel) > 100:\n",
    "                paraTMPLabel = paraTMPLabel[:100]\n",
    "\n",
    "            paraID = termID\n",
    "            if paras.get(str(paraTMP)) == None:\n",
    "                paras[str(paraTMP)] = termID\n",
    "                sentenceTerms.append(tuple([paraID, str(paraTMPLabel), \"dev\", False]))\n",
    "                sentenceSubs.append(tuple([paraID, PARAGRAPH_ID]))\n",
    "                termID += 1\n",
    "            else:\n",
    "                paraID = paras.get(str(paraTMP))\n",
    "\n",
    "            spanList.append(tuple([paraID, len(docTMP) + 1, len(docTMP)+len(paraTMP), refID, paraSpan, paraTMP, False]))\n",
    "            intersecs = [[docSpan, paraSpan], [paraSpan, docSpan], [paraSpan, catSpan], \n",
    "                         [catSpan, paraSpan], [paraSpan, yearSpan], [yearSpan, paraSpan],\n",
    "                         [paraSpan, authorSpan], [authorSpan, paraSpan]]\n",
    "            intersecList.extend(intersecs)\n",
    "            \n",
    "            spanID += 1\n",
    "            \n",
    "        #if len(docTMP) > 0:\n",
    "        #    docTMP += \"\\n\" \n",
    "        docTMP += \" \" + paraTMP\n",
    "        \n",
    "    spanEnd = len(docTMP)\n",
    "        \n",
    "    spanList.append(tuple([docID, spanBegin, spanEnd, refID, docSpan, docTMP, False]))\n",
    "    spanList.append(tuple(([yearID, spanBegin, spanEnd, refID, yearSpan, docTMP, False])))\n",
    "    spanList.append(tuple(([categoryID, spanBegin, spanEnd, refID, catSpan, docTMP, False])))\n",
    "    spanList.append(tuple(([authorID, spanBegin, spanEnd, refID, authorSpan, docTMP, False])))\n",
    "    \n",
    "    refList.append([refID, docTMP])\n",
    "       \n",
    "    refID += 1\n",
    "    spanID += 1\n",
    "    if index % 500 == 0:\n",
    "        spanDF = spanDF.append(pd.DataFrame.from_records(spanList))\n",
    "        intersecDF = intersecDF.append(pd.DataFrame.from_records(intersecList))\n",
    "        refDF = refDF.append(pd.DataFrame.from_records(refList))\n",
    "        sentTermDF = sentTermDF.append(pd.DataFrame.from_records(sentenceTerms))\n",
    "        sentSubDF = sentSubDF.append(pd.DataFrame.from_records(sentenceSubs))\n",
    "\n",
    "        spanList.clear()\n",
    "        intersecList.clear()\n",
    "        refList.clear()\n",
    "        sentenceTerms.clear()\n",
    "        sentenceSubs.clear()\n",
    "        \n",
    "        sentTermDF = sentTermDF.rename(columns={0 : \"id\", 1 : \"name\", 2 : \"knowledge_base\", 3 : \"deleted\"})\n",
    "        sentSubDF = sentSubDF.rename(columns={0 : \"term\", 1 : \"parent\", 2 : \"representation\", 3 : \"confidence\"})\n",
    "        spanDF = spanDF.rename(columns={0 : \"term\", 1 : \"span_begin\", 2 : \"span_end\", 3 : \"reference\", 4 : \"id\", 5 : \"text\", 6 : \"deleted\"})\n",
    "        intersecDF = intersecDF.rename(columns={0 : \"span1\", 1 : \"span2\"})\n",
    "        refDF = refDF.rename(columns={0: \"id\", 1 : \"reftext\"})      \n",
    "        \n",
    "        sentTermDF.to_sql('terms', con=engine, if_exists='append', method='multi', index = False, chunksize = 100000)        \n",
    "        sentSubDF.to_sql('subsumptions', con=engine, if_exists='append', method='multi', index = False, chunksize = 100000)        \n",
    "        spanDF.to_sql('spans', con=engine, if_exists='append', method='multi', index = False, chunksize = 100000)\n",
    "        intersecDF.to_sql('intersections', con=engine, if_exists='append', method='multi', index = False, chunksize = 100000)        \n",
    "        refDF.to_sql('full_texts', con=engine, if_exists='append', method='multi', index = False, chunksize = 100000)\n",
    "        \n",
    "        sentTermDF = pd.DataFrame()\n",
    "        sentSubDF = pd.DataFrame()\n",
    "        spanDF = pd.DataFrame()\n",
    "        intersecDF = pd.DataFrame()\n",
    "        refDF = pd.DataFrame()\n",
    "        \n",
    "spanDF = spanDF.append(pd.DataFrame(spanList))\n",
    "intersecDF = intersecDF.append(pd.DataFrame(intersecList))\n",
    "refDF = refDF.append(pd.DataFrame(refList))\n",
    "sentTermDF = sentTermDF.append(pd.DataFrame.from_records(sentenceTerms))\n",
    "sentSubDF = sentSubDF.append(pd.DataFrame.from_records(sentenceSubs))\n",
    "\n",
    "sentTermDF = sentTermDF.rename(columns={0 : \"id\", 1 : \"name\", 2 : \"knowledge_base\", 3 : \"deleted\"})\n",
    "sentSubDF = sentSubDF.rename(columns={0 : \"term\", 1 : \"parent\", 2 : \"representation\", 3 : \"confidence\"})\n",
    "spanDF = spanDF.rename(columns={0 : \"term\", 1 : \"span_begin\", 2 : \"span_end\", 3 : \"reference\", 4 : \"id\", 5 : \"text\", 6 : \"deleted\"})\n",
    "intersecDF = intersecDF.rename(columns={0 : \"span1\", 1 : \"span2\"})\n",
    "refDF = refDF.rename(columns={0: \"id\", 1 : \"reftext\"})\n",
    "sentTermDF.to_sql('terms', con=engine, if_exists='append', method='multi', index = False)\n",
    "sentSubDF.to_sql('subsumptions', con=engine, if_exists='append', method='multi', index = False, chunksize = 100000)\n",
    "spanDF.to_sql('spans', con=engine, if_exists='append', method='multi', index = False, chunksize = 100000)\n",
    "intersecDF.to_sql('intersections', con=engine, if_exists='append', method='multi', index = False, chunksize = 100000)\n",
    "refDF.to_sql('full_texts', con=engine, if_exists='append', method='multi', index = False, chunksize = 100000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
